{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AllisonDusek/HelloAI_/blob/main/softmax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "_oG7fMFUnQBK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a253a175-d6ec-46df-a515-c477bd71e56d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'nn3' already exists and is not an empty directory.\n",
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ],
      "source": [
        "#GOAL: use softmax to demonstrate that input images that are hard to classify produce an even 10-split of outputs\n",
        "\n",
        "# https://blog.infuseai.io/run-a-full-tty-terminal-in-google-colab-without-colab-pro-2759b9f8a74a\n",
        "# xterm may be helpful for speeding up some of the parts of this assignment ... see in class instruction on usage\n",
        "#!pip install colab-xterm\n",
        "#%load_ext colabxterm\n",
        "!git clone \"https://github.com/kartoone/nn3\"\n",
        "!git pull"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd nn3/src\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLG_Qmpb5jch",
        "outputId": "635f19fd-1ccf-49bc-a874-ff5a88188187"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nn3/src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from network2 import Network\n",
        "import mnist_loader\n",
        "import numpy as np\n",
        "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
        "net = Network([784, 30, 10])\n",
        "print(np.shape(net.biases))\n",
        "print(net.biases)\n"
      ],
      "metadata": {
        "id": "IoLOvM6M3XyO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75f4f20f-55ac-4cfb-ea7f-88c240e68efa"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2,)\n",
            "[array([[ 1.23700382],\n",
            "       [-0.35797896],\n",
            "       [-0.05310045],\n",
            "       [-0.29048455],\n",
            "       [ 1.13094651],\n",
            "       [ 0.17553334],\n",
            "       [ 0.92562344],\n",
            "       [ 0.09372741],\n",
            "       [-0.57854648],\n",
            "       [ 0.27747417],\n",
            "       [-0.43949823],\n",
            "       [-1.17065078],\n",
            "       [-1.71719769],\n",
            "       [-0.88986893],\n",
            "       [ 1.23979986],\n",
            "       [ 0.83746568],\n",
            "       [ 0.7968916 ],\n",
            "       [ 0.99567618],\n",
            "       [ 0.14150677],\n",
            "       [ 2.10530862],\n",
            "       [-0.07046346],\n",
            "       [-0.05699249],\n",
            "       [-0.2947701 ],\n",
            "       [ 0.25721085],\n",
            "       [-1.50523444],\n",
            "       [-0.49097975],\n",
            "       [-1.96445424],\n",
            "       [-1.60802055],\n",
            "       [-1.2496294 ],\n",
            "       [ 0.0886636 ]]), array([[ 0.41071083],\n",
            "       [-2.23017963],\n",
            "       [ 0.29240387],\n",
            "       [-0.5650161 ],\n",
            "       [ 0.43341617],\n",
            "       [-0.09647998],\n",
            "       [ 0.40781662],\n",
            "       [-0.00863538],\n",
            "       [-0.87345438],\n",
            "       [-1.38313261]])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:1970: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  result = asarray(a).shape\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# finds the image that the network has the hardest time classifying\n",
        "def findTroublesomeImage(test_data, net):\n",
        "  worsti = -1\n",
        "  worsta = 1.0\n",
        "  for i,(x,y) in enumerate(test_data):\n",
        "    a = net.feedforward(x)\n",
        "    if np.max(a)<worsta:\n",
        "      worsta = np.max(a)\n",
        "      worsti = i\n",
        "  print(worsta)\n",
        "  print(worsti)\n",
        "  return worsti\n"
      ],
      "metadata": {
        "id": "xOhqirWyX-1v"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  from network import Network\n",
        "  import mnist_loader\n",
        "  import time\n",
        "\n",
        "  training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
        "  configs = [[784,15,10], [784, 30, 10], [784, 100, 10], [784, 15, 15, 10], ] # ADD THE REMAINING CONFIGS HERE!!!!training_times = []\n",
        "  training_times = []\n",
        "  accuracies = []\n",
        "  for config in configs:\n",
        "    print(config)\n",
        "    start = time.time()\n",
        "    net = Network(config)\n",
        "    accuracies.append(net.SGD(training_data[0:100], 30, 10, 3.0, test_data=test_data[0:100]))\n",
        "    finish = time.time()\n",
        "    elapsed = finish - start\n",
        "    print(elapsed)\n",
        "    training_times.append(elapsed)\n",
        "    ii = findTroublesomeImage(test_data[0:100], net)\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.imshow(255*np.reshape(test_data[ii][0],(28,28)),cmap=\"gray_r\")\n",
        "    plt.show()\n",
        "# repeat the above IN DIFFERENT CELLS (don't do it all in one cell, you will thank me later) for all the structural configurations\n"
      ],
      "metadata": {
        "id": "k0DPyyG75pBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(configs)\n",
        "print(training_times)\n",
        "print(accuracies)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBZnlIf_azmy",
        "outputId": "cef83d2c-ce0f-47fc-9696-9c14152a71f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[784, 15, 10], [784, 30, 10], [784, 100, 10], [784, 15, 15, 10]]\n",
            "[0.5388364791870117, 0.5835187435150146, 1.0525076389312744, 0.6468594074249268]\n",
            "[35, 46, 31, 42]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  from network2 import Network\n",
        "  from network2 import QuadraticCost\n",
        "  import mnist_loader\n",
        "  import time\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
        "  configs = []\n",
        "  for epochs in [15, 30, 45, 100]:\n",
        "    for batchsize in [5, 10, 20, 100]:\n",
        "      for learningrate in [0.1, 0.5, 1.0, 3.0, 5.0]:\n",
        "        configs.append([epochs, batchsize, learningrate])\n",
        "\n",
        "  # only train on small slice (start with 100 ... but then once you have your graphs working ... bump it up to 1000 and re-run)\n",
        "  training_data = training_data[0:100]\n",
        "  test_data = test_data[0:100]\n",
        "\n",
        "  training_times = []\n",
        "  accuracies = []  \n",
        "  # note that this will be a GIANT list of training times for each config plus the two cost functions. [time1, time2, time3, time4] ... \n",
        "  #  time1 and time2 are both associated with the single hidden layer network and differ in the cost function (time1 - crossentropy, time2-quadratic)\n",
        "  #  time3 and time4 are both associated with the dual hidden layer network and differ in the cost function (time3 - crossentropy, time4-quadratic)\n",
        "  # NOTE: once you are happy with the results ... remove the 0:1000 and use the complete training data\n",
        "  for config in configs:\n",
        "    print(config, \"single hidden layer plus cross-entropy\")\n",
        "    start = time.time()\n",
        "    net1 = Network([784,15,10]) # cross-entropy cost\n",
        "    accuracies.append(net1.SGD(training_data[0:100], config[0], config[1], config[2], evaluation_data=test_data[0:100], monitor_evaluation_accuracy=True))\n",
        "    finish = time.time()\n",
        "    elapsed = finish - start\n",
        "    print(elapsed)\n",
        "    training_times.append(elapsed)\n",
        "    ii = findTroublesomeImage(test_data[0:100], net1)\n",
        "    plt.imshow(255*np.reshape(test_data[ii][0],(28,28)),cmap=\"gray_r\")\n",
        "    plt.show()\n",
        "\n",
        "    print(config, \"single hidden layer plus quadratic\")\n",
        "    start = time.time()\n",
        "    net1 = Network([784, 15, 10],cost=QuadraticCost) # quadratic cost\n",
        "    accuracies.append(net1.SGD(training_data[0:100], config[0], config[1], config[2], evaluation_data=test_data[0:100], monitor_evaluation_accuracy=True))\n",
        "    finish = time.time()\n",
        "    elapsed = finish - start\n",
        "    print(elapsed)\n",
        "    training_times.append(elapsed)\n",
        "    ii = findTroublesomeImage(test_data[0:100], net1)\n",
        "    plt.imshow(255*np.reshape(test_data[ii][0],(28,28)),cmap=\"gray_r\")\n",
        "    plt.show()\n",
        "\n",
        "    # repeat for the two layer network\n",
        "    print(config, \"dual hidden layers plus cross-entropy\")\n",
        "    start = time.time()\n",
        "    net2 = Network([784,30,15,10])\n",
        "    accuracies.append(net2.SGD(training_data[0:100], config[0], config[1], config[2], evaluation_data=test_data[0:100], monitor_evaluation_accuracy=True))\n",
        "    finish = time.time()\n",
        "    elapsed = finish - start\n",
        "    print(elapsed)\n",
        "    training_times.append(elapsed)\n",
        "    ii = findTroublesomeImage(test_data[0:100], net2)\n",
        "    plt.imshow(255*np.reshape(test_data[ii][0],(28,28)),cmap=\"gray_r\")\n",
        "    plt.show()\n",
        "\n",
        "    print(config, \"dual hidden layers plus quadratic\")\n",
        "    start = time.time()\n",
        "    net2 = Network([784,30,15, 10],cost=QuadraticCost)\n",
        "    accuracies.append(net2.SGD(training_data[0:100], config[0], config[1], config[2], evaluation_data=test_data[0:100], monitor_evaluation_accuracy=True))\n",
        "    finish = time.time()\n",
        "    elapsed = finish - start\n",
        "    print(elapsed)\n",
        "    training_times.append(elapsed)\n",
        "    ii = findTroublesomeImage(test_data[0:100], net2)\n",
        "    plt.imshow(255*np.reshape(test_data[ii][0],(28,28)),cmap=\"gray_r\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "Ypz37Ux0MAAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# pre-process our really janked up data ... first the training_times\n",
        "training_timesSingle1 = training_times[0::4]\n",
        "training_timesSingle2 = training_times[1::4]\n",
        "training_timesDouble1 = training_times[2::4]\n",
        "training_timesDouble2 = training_times[3::4]\n",
        "training_timesSingle = []\n",
        "training_timesDouble = []\n",
        "for tt1, tt2 in zip(training_timesSingle1, training_timesSingle2):\n",
        "    training_timesSingle.append(tt1)\n",
        "    training_timesSingle.append(tt2)\n",
        "for tt1, tt2 in zip(training_timesDouble1, training_timesDouble2):\n",
        "    training_timesDouble.append(tt1)\n",
        "    training_timesDouble.append(tt2)\n",
        "\n",
        "# and now the accuracies...\n",
        "accuraciesSingle1 = accuracies[0::4]\n",
        "accuraciesSingle2 = accuracies[1::4]\n",
        "accuraciesDouble1 = accuracies[2::4]\n",
        "accuraciesDouble2 = accuracies[3::4]\n",
        "accuraciesSingle = []\n",
        "accuraciesDouble = []\n",
        "for acc1, acc2 in zip(accuraciesSingle1, accuraciesSingle2):\n",
        "    accuraciesSingle.append(acc1[1][-1])\n",
        "    accuraciesSingle.append(acc2[1][-1])\n",
        "for acc1, acc2 in zip(accuraciesDouble1, accuraciesDouble2):\n",
        "    accuraciesDouble.append(acc1[1][-1])\n",
        "    accuraciesDouble.append(acc2[1][-1])\n",
        "\n",
        "# and now the labels\n",
        "labelsSingle = []\n",
        "labelsDouble = []\n",
        "for config in configs:\n",
        "    labelsSingle.append(f\"{config}-cross-784-15-10\")\n",
        "    labelsSingle.append(f\"{config}-quad-784-15-10\")\n",
        "    labelsDouble.append(f\"{config}-cross-784-30-15-10\")\n",
        "    labelsDouble.append(f\"{config}-quad-784-30-15-10\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(50,10))\n",
        "ax.plot(labelsSingle, accuraciesSingle)\n",
        "plt.show()\n",
        "fig, ax = plt.subplots(figsize=(50,10))\n",
        "ax.plot(labelsDouble, training_timesDouble)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fs2MWqSbsc_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract the \"results\" (training times and accuracies) for all configs where the # epochs is 30, the batch size is 10, the cost function is quadratic and the learning rate is variable\n",
        "label_learningrate = []\n",
        "tt_learningrate = []\n",
        "acc_learningrate = []\n",
        "for i, config in enumerate(configs):\n",
        "  if config[0] == 30 and config[1] == 10:\n",
        "    label_learningrate.append(labelsSingle[2*i])\n",
        "    tt_learningrate.append(training_timesSingle[2*i]) # 2i ... b/c we are storing two times for every config ... +1 b/c it's the second one that we want (i.e., the quadratic cost function)\n",
        "    acc_learningrate.append(accuraciesSingle[2*i]) # 2i ... b/c we are storing two times for every config ... +1 b/c it's the second one that we want (i.e., the quadratic cost function)\n",
        "\n",
        "print(label_learningrate)\n",
        "print(tt_learningrate)\n",
        "print(acc_learningrate)\n",
        "\n",
        "plt.subplots(figsize=(20,6))\n",
        "plt.plot(label_learningrate, tt_learningrate)\n",
        "plt.show()\n",
        "plt.subplots(figsize=(20,6))\n",
        "plt.plot(label_learningrate, acc_learningrate)\n",
        "plt.show"
      ],
      "metadata": {
        "id": "JnmRH5mMCFb9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}